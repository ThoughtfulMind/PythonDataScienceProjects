{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#Published on December 20, 2022 by Mar√≠lia Prata, mpwolke","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-20T22:18:37.389128Z","iopub.execute_input":"2022-12-20T22:18:37.389782Z","iopub.status.idle":"2022-12-20T22:18:37.797125Z","shell.execute_reply.started":"2022-12-20T22:18:37.389679Z","shell.execute_reply":"2022-12-20T22:18:37.795792Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\"A collection of tweets with the hashtag #chatgpt : discussions about the chatgpt language model, sharing experiences with using chatgpt, or asking for help with chatgpt-related issues. The tweets could also include links to articles or websites related to chatgpt, as well as images, videos, or other media. Overall, a collection of tweets with the hashtag #chatgpt would provide a glimpse into the online conversation surrounding chatgpt.\"\n\nhttps://www.kaggle.com/datasets/konradb/chatgpt-the-tweets","metadata":{}},{"cell_type":"markdown","source":"![](https://age-of-product.com/wp-content/uploads/Generative-AI-OpenAi-ChatGPT-Business-Agility-Scrum-Age-of-Product-com-1200x900-cropped.jpg)https://age-of-product.com/business-agility-scrum-generative-ai/","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/chatgpt-the-tweets/tweets.csv', encoding='utf8')\ndf.tail(2)","metadata":{"execution":{"iopub.status.busy":"2022-12-20T22:22:38.6014Z","iopub.execute_input":"2022-12-20T22:22:38.601957Z","iopub.status.idle":"2022-12-20T22:22:38.882008Z","shell.execute_reply.started":"2022-12-20T22:22:38.601928Z","shell.execute_reply":"2022-12-20T22:22:38.881273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport string\n\nimport nltk\nfrom nltk.probability import FreqDist\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom nltk import pos_tag\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\nfrom wordcloud import WordCloud\nfrom tqdm.auto import tqdm\nimport matplotlib.style as style\nstyle.use('fivethirtyeight')","metadata":{"execution":{"iopub.status.busy":"2022-12-20T22:22:44.028434Z","iopub.execute_input":"2022-12-20T22:22:44.02878Z","iopub.status.idle":"2022-12-20T22:22:44.34248Z","shell.execute_reply.started":"2022-12-20T22:22:44.028722Z","shell.execute_reply":"2022-12-20T22:22:44.3415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking dataset\n\nprint (\"Rows     : \" ,df.shape[0])\nprint (\"Columns  : \" ,df.shape[1])\nprint (\"\\nFeatures : \\n\" ,df.columns.tolist())\nprint (\"\\nMissing values :  \", df.isnull().sum().values.sum())\nprint (\"\\nUnique values :  \\n\",df.nunique())","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-12-20T22:22:49.12857Z","iopub.execute_input":"2022-12-20T22:22:49.128986Z","iopub.status.idle":"2022-12-20T22:22:49.237722Z","shell.execute_reply.started":"2022-12-20T22:22:49.128954Z","shell.execute_reply":"2022-12-20T22:22:49.236484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Users and followers","metadata":{}},{"cell_type":"code","source":"#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n\nprint(len(df[df['user_followers'] < 1000]), 'users with less than 1000 followers')\nprint(len(df[df['user_followers'] > 1000]), 'userss with more than 1000 followers')","metadata":{"execution":{"iopub.status.busy":"2022-12-20T22:23:02.164019Z","iopub.execute_input":"2022-12-20T22:23:02.164353Z","iopub.status.idle":"2022-12-20T22:23:02.183806Z","shell.execute_reply.started":"2022-12-20T22:23:02.164326Z","shell.execute_reply":"2022-12-20T22:23:02.182978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#The Hindu: User with more followers","metadata":{}},{"cell_type":"code","source":"#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n\n# User with the most followers\n\ndf[df['user_followers'] == df['user_followers'].max()]['user_name'].iloc[0]","metadata":{"execution":{"iopub.status.busy":"2022-12-20T22:23:07.005798Z","iopub.execute_input":"2022-12-20T22:23:07.006147Z","iopub.status.idle":"2022-12-20T22:23:07.0142Z","shell.execute_reply.started":"2022-12-20T22:23:07.006116Z","shell.execute_reply":"2022-12-20T22:23:07.012868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#The Hindu (7924486.0 followers)","metadata":{}},{"cell_type":"code","source":"hin = df[(df['user_name']==\"The Hindu\"\n         )].reset_index(drop=True)\nhin.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-20T22:23:11.719211Z","iopub.execute_input":"2022-12-20T22:23:11.720375Z","iopub.status.idle":"2022-12-20T22:23:11.738997Z","shell.execute_reply.started":"2022-12-20T22:23:11.720339Z","shell.execute_reply":"2022-12-20T22:23:11.738178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#User with more Friends: Jeff Sheehan Author | Mktg Consultant | Speaker","metadata":{}},{"cell_type":"code","source":"#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n\n# video with the most comments\n\ndf[df['user_friends'] == df['user_friends'].max()]['user_name'].iloc[0]","metadata":{"execution":{"iopub.status.busy":"2022-12-20T22:23:16.78616Z","iopub.execute_input":"2022-12-20T22:23:16.786514Z","iopub.status.idle":"2022-12-20T22:23:16.796179Z","shell.execute_reply.started":"2022-12-20T22:23:16.786487Z","shell.execute_reply":"2022-12-20T22:23:16.794996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Cleaning functions","metadata":{}},{"cell_type":"code","source":"#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n\ndef remove_line_breaks(text):\n    text = text.replace('\\r', ' ').replace('\\n', ' ')\n    return text\n\n#remove punctuation\ndef remove_punctuation(text):\n    re_replacements = re.compile(\"__[A-Z]+__\")  # such as __NAME__, __LINK__\n    re_punctuation = re.compile(\"[%s]\" % re.escape(string.punctuation))\n    '''Escape all the characters in pattern except ASCII letters and numbers'''\n    tokens = word_tokenize(text)\n    tokens_zero_punctuation = []\n    for token in tokens:\n        if not re_replacements.match(token):\n            token = re_punctuation.sub(\" \", token)\n        tokens_zero_punctuation.append(token)\n    return ' '.join(tokens_zero_punctuation)\n\ndef remove_special_characters(text):\n    text = re.sub('[^a-zA-z0-9\\s]', '', text)\n    return text\n\ndef lowercase(text):\n    text_low = [token.lower() for token in word_tokenize(text)]\n    return ' '.join(text_low)\n\ndef remove_stopwords(text):\n    stop = set(stopwords.words('english'))\n    word_tokens = nltk.word_tokenize(text)\n    text = \" \".join([word for word in word_tokens if word not in stop])\n    return text\n\n#remobe one character words\ndef remove_one_character_words(text):\n    '''Remove words from dataset that contain only 1 character'''\n    text_high_use = [token for token in word_tokenize(text) if len(token)>1]      \n    return ' '.join(text_high_use)   \n    \n#%%\n# Stemming with 'Snowball stemmer\" package\ndef stem(text):\n    stemmer = nltk.stem.snowball.SnowballStemmer('english')\n    text_stemmed = [stemmer.stem(token) for token in word_tokenize(text)]        \n    return ' '.join(text_stemmed)\n\ndef lemma(text):\n    wordnet_lemmatizer = WordNetLemmatizer()\n    word_tokens = nltk.word_tokenize(text)\n    text_lemma = \" \".join([wordnet_lemmatizer.lemmatize(word) for word in word_tokens])       \n    return ' '.join(text_lemma)\n\n\n#break sentences to individual word list\ndef sentence_word(text):\n    word_tokens = nltk.word_tokenize(text)\n    return word_tokens\n#break paragraphs to sentence token \ndef paragraph_sentence(text):\n    sent_token = nltk.sent_tokenize(text)\n    return sent_token    \n\n\ndef tokenize(text):\n    \"\"\"Return a list of words in a text.\"\"\"\n    return re.findall(r'\\w+', text)\n\ndef remove_numbers(text):\n    no_nums = re.sub(r'\\d+', '', text)\n    return ''.join(no_nums)\n\n\n\ndef clean_text(text):\n    _steps = [\n    remove_line_breaks,\n    remove_one_character_words,\n    remove_special_characters,\n    lowercase,\n    remove_punctuation,\n    remove_stopwords,\n    stem,\n    remove_numbers\n]\n    for step in _steps:\n        text=step(text)\n    return text   \n#%%","metadata":{"execution":{"iopub.status.busy":"2022-12-20T22:23:21.659847Z","iopub.execute_input":"2022-12-20T22:23:21.66018Z","iopub.status.idle":"2022-12-20T22:23:21.676031Z","shell.execute_reply.started":"2022-12-20T22:23:21.660149Z","shell.execute_reply":"2022-12-20T22:23:21.674925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#https://stackoverflow.com/questions/55557004/getting-attributeerror-float-object-has-no-attribute-replace-error-while\n#To avoid with tqdm AttributeError: 'float' object has no attribute\n\ndf[\"text\"] = df[\"text\"].astype(str)\ndf[\"text\"] = [x.replace(':',' ') for x in df[\"text\"]]","metadata":{"execution":{"iopub.status.busy":"2022-12-20T22:23:27.829075Z","iopub.execute_input":"2022-12-20T22:23:27.830013Z","iopub.status.idle":"2022-12-20T22:23:27.859336Z","shell.execute_reply.started":"2022-12-20T22:23:27.829981Z","shell.execute_reply":"2022-12-20T22:23:27.858434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['clean_text'] = pd.Series([clean_text(i) for i in tqdm(df['text'])])","metadata":{"execution":{"iopub.status.busy":"2022-12-20T22:23:31.881204Z","iopub.execute_input":"2022-12-20T22:23:31.881539Z","iopub.status.idle":"2022-12-20T22:24:20.530588Z","shell.execute_reply.started":"2022-12-20T22:23:31.881513Z","shell.execute_reply":"2022-12-20T22:24:20.529133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words = df[\"clean_text\"].values","metadata":{"execution":{"iopub.status.busy":"2022-12-20T22:24:25.156103Z","iopub.execute_input":"2022-12-20T22:24:25.15645Z","iopub.status.idle":"2022-12-20T22:24:25.162141Z","shell.execute_reply.started":"2022-12-20T22:24:25.156423Z","shell.execute_reply":"2022-12-20T22:24:25.160802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n\nls = []\n\nfor i in words:\n    ls.append(str(i))","metadata":{"execution":{"iopub.status.busy":"2022-12-20T22:24:30.906479Z","iopub.execute_input":"2022-12-20T22:24:30.906831Z","iopub.status.idle":"2022-12-20T22:24:30.925648Z","shell.execute_reply.started":"2022-12-20T22:24:30.906799Z","shell.execute_reply":"2022-12-20T22:24:30.924608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls[:5]","metadata":{"execution":{"iopub.status.busy":"2022-12-20T22:24:35.134224Z","iopub.execute_input":"2022-12-20T22:24:35.135307Z","iopub.status.idle":"2022-12-20T22:24:35.140339Z","shell.execute_reply.started":"2022-12-20T22:24:35.135276Z","shell.execute_reply":"2022-12-20T22:24:35.139554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n\n# The wordcloud \nplt.figure(figsize=(16,13))\nwc = WordCloud(background_color=\"lightblue\", colormap='Set2', max_words=1000, max_font_size= 200,  width=1600, height=800)\nwc.generate(\" \".join(ls))\nplt.title(\"Most discussed terms\", fontsize=20)\nplt.imshow(wc.recolor( colormap= 'Set2' , random_state=17), alpha=0.98, interpolation=\"bilinear\", )\nplt.axis('off')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-12-20T22:24:40.31186Z","iopub.execute_input":"2022-12-20T22:24:40.312868Z","iopub.status.idle":"2022-12-20T22:24:50.909893Z","shell.execute_reply.started":"2022-12-20T22:24:40.312816Z","shell.execute_reply":"2022-12-20T22:24:50.908854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#I don't have (explicit) target. Then I'll try some of the features.","metadata":{}},{"cell_type":"code","source":"#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n\nmost_pop = df.sort_values('user_followers', ascending =False)[['user_name', 'user_followers']].head(12)\n\nmost_pop['user_followers1'] = most_pop['user_followers']/1000","metadata":{"execution":{"iopub.status.busy":"2022-12-20T22:25:22.615879Z","iopub.execute_input":"2022-12-20T22:25:22.616221Z","iopub.status.idle":"2022-12-20T22:25:22.659274Z","shell.execute_reply.started":"2022-12-20T22:25:22.616188Z","shell.execute_reply":"2022-12-20T22:25:22.658315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Since text delivered tiny fontsize that I couldn't read I changed to user name","metadata":{}},{"cell_type":"code","source":"#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n\nplt.figure(figsize = (20,25))\n\nsns.barplot(data = most_pop, y = 'user_name', x = 'user_followers1', color = 'c')\nplt.xticks(fontsize=27, rotation=0)\nplt.yticks(fontsize=30, rotation=0)\nplt.xlabel('User followers in Thousands', fontsize = 21)\nplt.ylabel('')\nplt.title('Followers', fontsize = 30);","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-12-20T22:25:27.014592Z","iopub.execute_input":"2022-12-20T22:25:27.01549Z","iopub.status.idle":"2022-12-20T22:25:27.41953Z","shell.execute_reply.started":"2022-12-20T22:25:27.015437Z","shell.execute_reply":"2022-12-20T22:25:27.418474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Dane Cook??? Really Dane Cook?\n\n\"Dane Jeffrey Cook is an American stand-up comedian and film actor.   He performed an HBO special in late 2006, Vicious Circle, a straight-to-DVD special titled Rough Around The Edges (which is included in the album of the same name), and a Comedy Central special in 2009 titled Isolated Incident. He is known for his use of observational, often vulgar, and sometimes dark comedy.\"\n\nhttps://en.wikipedia.org/wiki/Dane_Cook","metadata":{}},{"cell_type":"code","source":"import gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nimport numpy as np\nnp.random.seed(2018)\nimport nltk","metadata":{"execution":{"iopub.status.busy":"2022-12-20T22:25:41.284852Z","iopub.execute_input":"2022-12-20T22:25:41.285184Z","iopub.status.idle":"2022-12-20T22:25:41.37352Z","shell.execute_reply.started":"2022-12-20T22:25:41.285158Z","shell.execute_reply":"2022-12-20T22:25:41.372882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stemmer = SnowballStemmer('english')","metadata":{"execution":{"iopub.status.busy":"2022-12-20T22:25:45.642602Z","iopub.execute_input":"2022-12-20T22:25:45.644021Z","iopub.status.idle":"2022-12-20T22:25:45.649133Z","shell.execute_reply.started":"2022-12-20T22:25:45.643949Z","shell.execute_reply":"2022-12-20T22:25:45.648175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('wordnet')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-12-20T22:25:50.015986Z","iopub.execute_input":"2022-12-20T22:25:50.016346Z","iopub.status.idle":"2022-12-20T22:25:50.238168Z","shell.execute_reply.started":"2022-12-20T22:25:50.016317Z","shell.execute_reply":"2022-12-20T22:25:50.237325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n\ndef lemmatize_stemming(text):\n    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n\ndef preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            result.append(lemmatize_stemming(token))\n    return result","metadata":{"execution":{"iopub.status.busy":"2022-12-20T22:25:55.359692Z","iopub.execute_input":"2022-12-20T22:25:55.36069Z","iopub.status.idle":"2022-12-20T22:25:55.366239Z","shell.execute_reply.started":"2022-12-20T22:25:55.360661Z","shell.execute_reply":"2022-12-20T22:25:55.365214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'].iloc[2]","metadata":{"execution":{"iopub.status.busy":"2022-12-20T22:26:00.091089Z","iopub.execute_input":"2022-12-20T22:26:00.09191Z","iopub.status.idle":"2022-12-20T22:26:00.098131Z","shell.execute_reply.started":"2022-12-20T22:26:00.091874Z","shell.execute_reply":"2022-12-20T22:26:00.097032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('omw-1.4')","metadata":{"execution":{"iopub.status.busy":"2022-12-20T22:26:52.964777Z","iopub.execute_input":"2022-12-20T22:26:52.965116Z","iopub.status.idle":"2022-12-20T22:26:53.18457Z","shell.execute_reply.started":"2022-12-20T22:26:52.965089Z","shell.execute_reply":"2022-12-20T22:26:53.183699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n\ndoc_sample = df['text'].iloc[1]\nprint('original document: ')\n\nwords = []\n\nfor word in doc_sample.split(' '):\n    words.append(word)\n    \n    \nprint(words)\nprint('\\n\\n tokenized and lemmatized document: ')\nprint(preprocess(doc_sample))","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-12-20T22:27:00.056919Z","iopub.execute_input":"2022-12-20T22:27:00.057278Z","iopub.status.idle":"2022-12-20T22:27:01.704417Z","shell.execute_reply.started":"2022-12-20T22:27:00.057249Z","shell.execute_reply":"2022-12-20T22:27:01.703579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['clean_text'] = df['clean_text'].astype(str)","metadata":{"execution":{"iopub.status.busy":"2022-12-20T22:27:10.484206Z","iopub.execute_input":"2022-12-20T22:27:10.484601Z","iopub.status.idle":"2022-12-20T22:27:10.4944Z","shell.execute_reply.started":"2022-12-20T22:27:10.484567Z","shell.execute_reply":"2022-12-20T22:27:10.493234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n\nwords = []\n\nfor i in df['clean_text']:\n        words.append(i.split(' '))","metadata":{"execution":{"iopub.status.busy":"2022-12-20T22:27:14.683431Z","iopub.execute_input":"2022-12-20T22:27:14.683734Z","iopub.status.idle":"2022-12-20T22:27:14.742158Z","shell.execute_reply.started":"2022-12-20T22:27:14.683711Z","shell.execute_reply":"2022-12-20T22:27:14.740771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Create the dictionary\n\nEvery unique word in texts","metadata":{}},{"cell_type":"code","source":"#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n\ndictionary = gensim.corpora.Dictionary(words)\n\ncount = 0\nfor k, v in dictionary.iteritems():\n    print(k, v)\n    count += 1\n    if count > 10:\n        break","metadata":{"execution":{"iopub.status.busy":"2022-12-20T22:27:20.356424Z","iopub.execute_input":"2022-12-20T22:27:20.356784Z","iopub.status.idle":"2022-12-20T22:27:21.395218Z","shell.execute_reply.started":"2022-12-20T22:27:20.356739Z","shell.execute_reply":"2022-12-20T22:27:21.394123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filter out tokens in the dictionary by their frequency.\n\ndictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)","metadata":{"execution":{"iopub.status.busy":"2022-12-20T22:27:27.41913Z","iopub.execute_input":"2022-12-20T22:27:27.419438Z","iopub.status.idle":"2022-12-20T22:27:27.707505Z","shell.execute_reply.started":"2022-12-20T22:27:27.419414Z","shell.execute_reply":"2022-12-20T22:27:27.706323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Create Corpus -> term document frequency\n\ndoc2bow() simply counts the number of occurrences of each distinct word, converts the word to its integer word ID and returns the result as a sparse vector.","metadata":{}},{"cell_type":"code","source":"bow_corpus = [dictionary.doc2bow(doc) for doc in words]\nbow_corpus[4310]","metadata":{"execution":{"iopub.status.busy":"2022-12-20T22:27:33.454699Z","iopub.execute_input":"2022-12-20T22:27:33.455075Z","iopub.status.idle":"2022-12-20T22:27:33.864293Z","shell.execute_reply.started":"2022-12-20T22:27:33.455047Z","shell.execute_reply":"2022-12-20T22:27:33.8636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n\nbow_doc_4310 = bow_corpus[4310]\n\nfor i in range(len(bow_doc_4310)):\n    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n                                               dictionary[bow_doc_4310[i][0]], \nbow_doc_4310[i][1]))","metadata":{"execution":{"iopub.status.busy":"2022-12-20T22:27:39.336232Z","iopub.execute_input":"2022-12-20T22:27:39.336561Z","iopub.status.idle":"2022-12-20T22:27:39.343772Z","shell.execute_reply.started":"2022-12-20T22:27:39.336534Z","shell.execute_reply":"2022-12-20T22:27:39.342634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#TF/IDF","metadata":{}},{"cell_type":"code","source":"#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n\nfrom gensim import corpora, models\n\ntfidf = models.TfidfModel(bow_corpus)\ncorpus_tfidf = tfidf[bow_corpus]\n\nfrom pprint import pprint\n\nfor doc in corpus_tfidf:\n    pprint(doc)\n    break","metadata":{"execution":{"iopub.status.busy":"2022-12-20T22:27:48.671558Z","iopub.execute_input":"2022-12-20T22:27:48.672643Z","iopub.status.idle":"2022-12-20T22:27:48.775024Z","shell.execute_reply.started":"2022-12-20T22:27:48.672588Z","shell.execute_reply":"2022-12-20T22:27:48.773812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n\nlda_model = gensim.models.LdaMulticore(bow_corpus,\n                                       num_topics=10,\n                                       id2word=dictionary,\n                                       passes=2,\n                                       workers=2)","metadata":{"execution":{"iopub.status.busy":"2022-12-20T22:27:54.937888Z","iopub.execute_input":"2022-12-20T22:27:54.938252Z","iopub.status.idle":"2022-12-20T22:28:09.110352Z","shell.execute_reply.started":"2022-12-20T22:27:54.938224Z","shell.execute_reply":"2022-12-20T22:28:09.109115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Show the output of the model","metadata":{}},{"cell_type":"code","source":"#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n\nfor idx, topic in lda_model.print_topics(-1):\n    print('Topic: {} \\nWords: {}'.format(idx, topic))","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-12-20T22:28:20.092707Z","iopub.execute_input":"2022-12-20T22:28:20.093048Z","iopub.status.idle":"2022-12-20T22:28:20.102213Z","shell.execute_reply.started":"2022-12-20T22:28:20.093021Z","shell.execute_reply":"2022-12-20T22:28:20.101192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n\nlda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf,\n                                             num_topics=10,\n                                             id2word=dictionary,\n                                             passes=2,\n                                             workers=4)\n\nfor idx, topic in lda_model_tfidf.print_topics(-1):\n    print('Topic: {} Word: {}'.format(idx, topic))","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-12-20T22:28:37.11412Z","iopub.execute_input":"2022-12-20T22:28:37.114461Z","iopub.status.idle":"2022-12-20T22:28:46.611398Z","shell.execute_reply.started":"2022-12-20T22:28:37.114436Z","shell.execute_reply":"2022-12-20T22:28:46.609444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Sucker of ChatGPT","metadata":{}},{"cell_type":"code","source":"#39049th row, 2nd column \n\ndf.iloc[39049,1]","metadata":{"execution":{"iopub.status.busy":"2022-12-20T22:28:58.868673Z","iopub.execute_input":"2022-12-20T22:28:58.869032Z","iopub.status.idle":"2022-12-20T22:28:58.875539Z","shell.execute_reply.started":"2022-12-20T22:28:58.869004Z","shell.execute_reply":"2022-12-20T22:28:58.874665Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQeq2nhB-_7sZRyS6IV0ioDKXjfdA8YXHkCo-jMh82fi1DUuW8p5tca60ZBIQbmp1ebhtA&usqp=CAU)https://makeameme.org/meme/ha-ha-sucker","metadata":{}},{"cell_type":"markdown","source":"#Don't forget to add that snippet below to avoid errors!\n\nimport nltk\nnltk.download()","metadata":{"_kg_hide-output":true}},{"cell_type":"markdown","source":"#I couldn't fix the NLTK Downloader installation above to go further.","metadata":{}},{"cell_type":"code","source":"#Code by Leon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n\nunseen_document = 'so happy for the chatGPT team for com8ng up with such a revolutionary idea.The FUTURE LOOKS BRIGHT.'\nbow_vector = dictionary.doc2bow(preprocess(unseen_document))\n\nfor index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-12-20T22:31:33.17491Z","iopub.execute_input":"2022-12-20T22:31:33.175235Z","iopub.status.idle":"2022-12-20T22:31:33.184926Z","shell.execute_reply.started":"2022-12-20T22:31:33.175209Z","shell.execute_reply":"2022-12-20T22:31:33.184029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](https://res.cloudinary.com/practicaldev/image/fetch/s--Qnwum4Gg--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://i.redd.it/4e7ywl1kj74a1.png)https://dev.to/ben/meme-tuesday-5fdn/comments","metadata":{}},{"cell_type":"markdown","source":"#Acknowledgements:\n\nLeon Wolber https://www.kaggle.com/leonwolber/reddit-nlp-topic-modeling-prediction\n\nDataset by Konrad Banachewicz https://www.kaggle.com/datasets/konradb/chatgpt-the-tweets","metadata":{}}]}